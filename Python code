# Cell 1: Install all required libraries
!pip install git+https://github.com/openai/whisper.git -q
!pip install speechrecognition pydub sentence-transformers transformers torch -q

print("âœ… All libraries installed successfully!")
# Cell 2: Import libraries and load models
import speech_recognition as sr
from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode
import numpy as np
from pydub import AudioSegment
import io
import torch

# For Transcription (using OpenAI's Whisper)
import whisper

# For Emotion Analysis
from transformers import pipeline

# For Semantic Scoring
from sentence_transformers import SentenceTransformer, util

# --- Load Models ---
# This might take a few minutes the first time
print("Loading AI models, this may take a moment...")

# 1. Transcription Model (Whisper)
# 'tiny' is fast, 'base' is a good balance, 'medium' or 'large' are more accurate but slower.
transcription_model = whisper.load_model("base")

# 2. Emotion Analysis Model
emotion_classifier = pipeline("text-classification", model="cardiffnlp/twitter-roberta-base-emotion", top_k=1)

# 3. Semantic Similarity Model
scoring_model = SentenceTransformer('all-MiniLM-L6-v2')

print("âœ… All models loaded and ready!")
# Cell 3: JavaScript and Python helper function to record audio from the microphone

def record_audio(filename="audio.wav"):
  """
  Records audio from the browser microphone and saves it as a WAV file.
  """
  js = Javascript("""
    async function recordAudio() {
      const div = document.createElement('div');
      const startButton = document.createElement('button');
      startButton.textContent = 'Start Recording';
      div.appendChild(startButton);

      const stopButton = document.createElement('button');
      stopButton.textContent = 'Stop Recording';
      stopButton.disabled = true;
      div.appendChild(stopButton);

      const audio = document.createElement('audio');
      audio.controls = true;
      div.appendChild(audio);

      document.body.appendChild(div);

      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      let recorder;
      let chunks = [];

      startButton.onclick = () => {
        recorder = new MediaRecorder(stream);
        recorder.ondataavailable = e => chunks.push(e.data);
        recorder.start();
        startButton.disabled = true;
        stopButton.disabled = false;
        console.log("Recording started...");
      };

      stopButton.onclick = () => {
        recorder.onstop = () => {
          const blob = new Blob(chunks, { type: 'audio/webm' });
          chunks = [];
          const url = window.URL.createObjectURL(blob);
          audio.src = url;
          console.log("Recording stopped.");

          // Convert blob to base64 for Python
          const reader = new FileReader();
          reader.readAsDataURL(blob);
          reader.onloadend = () => {
            document.body.removeChild(div);
            // Resolve the promise with the base64 data
            window.audioData = reader.result;
          };
        };
        recorder.stop();
        startButton.disabled = false;
        stopButton.disabled = true;
      };

      // Wait for the stop button to be clicked by returning a promise
      await new Promise(resolve => stopButton.onclick = resolve);
      stopButton.onclick(); // Trigger the stop logic
    }
    recordAudio();
  """)
  display(js)

  # Wait for the JavaScript to finish and set window.audioData
  eval_js('new Promise(resolve => { const check = () => { if (window.audioData) { resolve(window.audioData) } else { setTimeout(check, 100) } }; check() })')

  # Decode the base64 audio and save
  data = eval_js('window.audioData')
  binary = b64decode(data.split(',')[1])

  # The browser records in webm, we convert it to wav for broad compatibility
  webm_audio = AudioSegment.from_file(io.BytesIO(binary), format="webm")
  webm_audio.export(filename, format="wav")
  print(f"Audio saved as {filename}")
  return filename
# Cell 5: The Main Interview Simulation

# Define your interview questions and ideal answers for scoring
interview_questions = {
    "Tell me about a time you faced a difficult challenge at work and how you handled it.": "An ideal answer would describe the STAR method: Situation, Task, Action, and Result. It should demonstrate problem-solving skills, resilience, and a positive outcome.",
    "What are your biggest strengths?": "A strong answer should mention strengths relevant to the job, like 'problem-solving', 'team collaboration', or 'quick learning', and provide a brief example for each.",
    "Why are you interested in this role?": "The best answers show genuine research into the company and role, connecting the candidate's skills and career goals to the company's mission and the job description."
}

# --- Let's Start the Interview ---
print("--- Welcome to the AI Virtual Interviewer ---")
print("You will be asked a few questions. Click 'Start Recording' to begin your answer and 'Stop Recording' when you are finished.\n")

total_score = 0
question_count = len(interview_questions)

for i, (question, ideal_answer) in enumerate(interview_questions.items()):
    print(f"\n--- Question {i+1}/{question_count} ---")
    print(f"Q: {question}\n")

    # 1. Record Answer
    print("Please record your answer now...")
    audio_file = record_audio()

    # 2. Process the answer through the AI pipeline
    user_transcription = transcribe_answer(audio_file)
    if not user_transcription.strip():
        print("\n--- FEEDBACK ---")
        print("I didn't catch that. Please try to speak clearly. Moving to the next question.")
        continue

    detected_emotion = analyze_emotion(user_transcription)
    relevance_score = score_relevance(user_transcription, ideal_answer)
    total_score += relevance_score

    # 3. Provide Feedback
    print("\n--- FEEDBACK ON YOUR ANSWER ---")
    print(f"ðŸ—£ï¸ Your Answer (Transcribed): \"{user_transcription}\"")
    print(f"ðŸ˜Š Detected Emotion: {detected_emotion}")
    print(f"ðŸŽ¯ Relevance Score: {relevance_score}/100")
    if relevance_score > 70:
        print("   Comment: Great! Your answer was highly relevant to the question.")
    elif relevance_score > 40:
        print("   Comment: Good attempt. Your answer was moderately relevant. Try to be more specific next time.")
    else:
        print("   Comment: Your answer seemed to be off-topic. Make sure to address the question directly.")
    print("-" * 30)

# --- Final Summary ---
print("\n\n--- INTERVIEW COMPLETE ---")
average_score = total_score / question_count
print(f"Your overall average relevance score was: {average_score:.2f}/100")
print("Thank you for participating!")
